[["index.html", "PO11Q: Seminar Companion Preface", " PO11Q: Seminar Companion Dr Flo Reiche Department of Politics and International Studies University of Warwick Last Updated 14 December, 2024 Preface A vast amount of political research is quantitative, and even if you decide never to conduct quantitative analysis yourself, you will find an introductory level of knowledge in quantitative methods useful to critically engage with your discipline as a whole. Skills in data analysis are also crucial for finding employment in graduate-level jobs. This module will deliver an introduction to quantitative political analysis. It is based on Reiche’s typology of Quantitative Methods, and explores each of its tasks (conceptualisation and measurement, numerical data, data analysis, and interpretation) at an introductory level. The module uses the software R. This is the online companion to the seminars on PO11Q. It hosts all of the material needed for the seminars and replaces the physical worksheets which we work through in some of the seminars. This is environmentally friendlier, and once we get to working with R, it also allows you to copy / paste code directly from the code chunks in this online companion. I hope you find this useful! "],["companion-features.html", "Companion Features", " Companion Features You will find embedded in the text four different types of boxes which serve different purposes: This box appears whenever I want you to stop at a particular point in the worksheet and to flag up to me that you are done. This appears when you need to be careful with your coding in R to avoid problems. Some explanations that will hopefully make your work with this webpage or learning the material itself easier. We will start working with R in week 5, and I have recorded some videos to ease you into working with the program. A brief question which tests your understanding of the previous material. A definition we encountered in the lecture. "],["accessibility.html", "Accessibility", " Accessibility For those of you who prefer a dark background, like me, you can select this option from the menu at the top of the page. Click the “A” symbol, and then you can choose between “white”, “sepia”, or “night”. "],["no-seminar.html", "No Seminar", " No Seminar As this is a first-year module, there is no seminar in Week 1. "],["introduction.html", "Introduction Introduction to Moodle Introduction to Talis Aspire Assessment A Simple Question (?) Next Week", " Introduction Introduction to Moodle The Global View The content of the module is organised into weekly folders The Weekly View In each folder, you will find the labels “Learning Outcomes” “Before the Lecture” “Lecture Materials” “Before the Seminar” “Seminar Materials”, “After the Seminar” “Useful Stuff” (sometimes) Learning Outcomes For each week you will find the learning outcomes at the top These relate to the lecture, the seminar, and the reading Lecture Here you will find the preparation you are required to complete BEFORE coming to the lecture. Here you will also find a link to the reading list detailing the readings you are expected to complete BEFORE the lecture. The slides for each lecture will be available here on the previous Friday (latest). Seminar Here you will find the preparation you are required to complete BEFORE coming to the seminar. Here you will also find a link to the reading list detailing the readings you are expected to complete BEFORE the seminar. The seminar reading is different from and additional to the lecture readings. The slides for each seminar will be available here on the previous Friday (latest). Seminar – Getting to Know R Once we start working with R, there will be little video clips introducing you to various bits and bops Which role does the quiz have? It is the formative assessment of the module As such, it gives you an overview of your progress It also gives Flo an overview of gaps and problematic areas The odd question might find its way into the exam It is non-assessed, but very important to complete What can I find under the label “Useful Stuff”? Do you know this one drawer in your house that contains all sorts of stuff? This is the “Useful Stuff” label The things in there are relevant for the topics we cover, but is complementary Fora There are a number of fora at the top of the Moodle page Please use them as intended Where can I find information on the assessment? The module has two assessments Information on these can be found in the “Assessment” folder on the Moodle page Introduction to Talis Aspire Talis Aspire is an online repository for the reading list It allows you to access resources that are available online (most are) directly How do I find a reading? Available through Moodle How do I find a reading? (contd.) Scroll to the appropriate item If you see an icon labelled on the right, click on it You will be asked to log in with your university credentials You can then click yourself through to the actual document What if the reading is not digitally available? Then, I’m afraid, you will have to go to the library in person Essential versus Recommended Reading Essential: This reading is compulsory, and the material covered in it is relevant for the exam. Recommended: If you want a different take on the same topic, or you want some more information, then read these materials. Does it make sense to buy a book? Yes, there are two core texts for this module: Fogarty (2023) Agresti (2018) Assessment Do you have any Questions? A Simple Question (?) Group Work 1 How would you measure the size of the European Union? Come together in small groups (3-4) Try to identify ways in which you would measure the size of the European Union Group Work 2 Consider the measurements you developed for the size of the EU Identify which of the items are attributes and which are measurements? Draw the conceptualisation tree and sort your items into attributes (branches) and measurements (leaves) Complete the tree so that all branches have leaves and and all leaves are attached to a branch. Prepare a short presentation of your results Next Week Homework for the Seminar, Week 3 Students as Learners: Read ALL the indicative items for the concept you chose for the assessment. Students as Researchers: Draw the conceptualisation tree for the concept you selected for the assessment Choose at least three attributes for this concept Choose two measurements per attribute "],["conceptualisation-and-measurement.html", "Conceptualisation and Measurement Self-Assessment Questions1 Assessment Assessment Guidelines Referencing Writing your Assessment Group Work Next Week", " Conceptualisation and Measurement Self-Assessment Questions1 How should you decide which attributes to include in your conceptualization of a concept during a research project? Give an example of a systematized concept. State its explicit definition and identify the attributes it contains. Why should you not be concerned with independent and dependent variables during conceptualization? Why is it important to ensure validity and reliability within your research? Give an example of a situation where you would have to do a conceptualization within a conceptualization. Please stop here and don’t go beyond this point until we have compared notes on your answers. Assessment Do you have any questions? Assessment Guidelines Assessment Guidelines The guidelines of the PAIS UG handbook apply to this module. The following regulations replace the corresponding rules in the handbook. All other guidelines in the UG handbook remain unaffected. Page one of the essay MUST include student ID, essay / dissertation title (exactly as previously submitted), total word count and word count for illustrations (see point 5). Use font size 11, line spacing 1.5 and justified alignment, in Arial, Times New Roman, or Calibri. If you are using Markdown or LaTeX then the default font CMU is also fine. Use Harvard style referencing, that is, “(name, date, p./pp. ##)” in the text, plus complete list of references at the end of the essay / dissertation. All references have to appear in the list of references, and all elements in the list of references have to appear at least once in the text. In-text references count towards the word length. Do not use footnotes for explanatory text. If you do so, these will count towards the word limit. A page counts as 400 words. Any table or figure counts proportionally with respect to the space it occupies. For example, a table or figure taking up half of a page counts as 200 words. Illustrations should not count for more than 30% of your whole essay / dissertation. The bibliography and appendix are not included in the word count. The penalty for exceeding the word limit will be a deduction of 3 marks for every ten percent (or part thereof) over the word limit specified in the assessment information on Moodle. The limit is absolute and there is no percentage leeway. DOES NOT APPLY TO YOU. THERE IS NO APPENDIX. The original wording is available here Referencing “The aim of a reference is simple: to provide the reader with sufficient information so that the sources that you have cited or quoted can be verified easily. The reader must be able to verify whether you have used or interpreted the source appropriately.” (PAIS Undergraduate Handbook, Essay Guide. Retrieved 20/10/17.) References: How? The expectation is that you provide at least ten unique references, and include material beyond the reading list. When should you include references? When you refer to an idea or argument that is not originally yours When you refer to statistics, data, or other claims (‘statements of fact’) When you are quoting someone The PAIS Undergraduate Handbook and the academic skills pages should always be your first port of call for this module. References: Harvard You can only use Harvard as referencing system: As Dahl (1989) argues, … The notion of who the people are in a democracy is not clearly defined (Dahl, 1989, p. 2) … Dahl, R. A. (1989). Democracy and its Critics. New Haven, CT: Yale University Press. Please remember that only in-text citations are permissible for this assessment. Quotations Quotation marks indicate the beginning and end of each quotation. Use double quotation marks for this purpose. But if the quotation contains a further quotation within itself, then use single quotation marks to indicate the inner quotation. Example: According to Parker (2000, p. 284), “An editorial from the Ontario Women’s Association Newsletter states that ‘There are no women’s issues, they should be called family issues’”. Long Quotations Quotations longer than 40 words are called block quotations and deserve special treatment. The quoted passage does not need quotation marks but it should be indented, i.e. the line length should be reduced by about 1cm at both ends. Bibliography vs. List of References References: Only contains the works cited in the text. Bibliography: Contains the works cited in the text as well asthe sources you have used for background reading without including them in your output The PAIS UG Handbook refers to it as a bibliography, but has a list of references in mind. You know better now. Please name it “References”. List of References Does not count towards the word limit Sorted alphabetically by surname of author No bullet points No numbering No creativity Writing your Assessment You are [becoming] a professional author. Learn to use the tools of authorship or choose a profession for which you are better suited. (Stimson, n.d.) I realise that you currently have a lot of things on your plate, such as settling into university life, and writing a very challenging assessment which is due even before reading week of your first term. However, academic writing is one of the many tools which you will have to learn over the course of your degree, and the professional presentation of your intellectual output is one of the many components of academic writing. Unfortunately, MS Word and Apple Pages fail to meet the required standards and so many political scientists (yours truly included) have switched to a program called LaTeX. To be clear: I have absolutely no expectation that you will use LaTex in the assessment. But using LaTeX will allow you to: submit a beautifully typeset document automate referencing and compilation of the list of references acquire transferable skills for academic writing that will serve you well for all assessments to come, including your dissertation If you are currently suffering from an overload of information, my advice would be to park this, and come back to it later if you are interested. You will retain access to this Moodle page throughout your studies at Warwick. But if you are intrigued, then please follow me to my “Academic Writing with LaTeX” module. The first two sections of this module will be sufficient for most if not all assessments. The page also contains an essay template which meets all of the formatting requirements I would like to see in an essay (regardless whether you submit this in MS Word, Pages, or any other format). Group Work Come together in your conceptual groups Draw one, big tree which contains all your attributes and measurements Prepare to present the results to the seminar Next Week Students as learners: Ensure that you have been reading widely around your concept (i.e. move beyond the sources on Moodle) Students as researchers: Prepare a list of the reasons which make your concept difficult to conceptualise and measure. Prepare a list of questions you have about the assessment. Some of the content of this worksheet is taken from Reiche (forthcoming).↩︎ "],["descriptive-statistics.html", "Descriptive Statistics Self-Assessment Questions2 Calculations Solutions", " Descriptive Statistics Self-Assessment Questions2 How can we distinguish between an attribute and a predictor? Why are descriptive statistics useful? Why do we need measures of centrality and spread? Why is the standard deviation more useful as a statistic than the deviation? Why is the sum of deviations always equal to zero? Please stop here and don’t go beyond this point until we have compared notes on your answers. Calculations Data Set Table 1: Data Set i income age 1 450 21 2 550 23 3 300 27 4 650 30 5 100 20 6 900 18 7 200 20 8 250 22 9 300 21 10 600 21 Exercises Calculate the following descriptive statistics for each variable: Mean Mode Median Range Standard Deviation Variance Solutions You can find the Solutions here or in the Downloads Section. Some of the content of this worksheet is taken from Reiche (forthcoming).↩︎ "],["introduction-to-r.html", "Introduction to R Self-Assessment Questions3 R &amp; RStudio – Installation R - Getting Started RScript First Steps in R The Working Directory R Packages Opening your Data Set Viewing the Data Variable Types in R Sub-Setting Data Ordering Data Grouping Data Combining Ordering and Grouping Data Saving Homework", " Introduction to R Self-Assessment Questions3 How do you calculate the relative frequencies for each of the categories of a variable? Give an example of a dichotomous variable. Why is age a continuous variable instead of a discrete variable? How does an interval scale differ from an ordinal scale? Explain using examples (not from the lecture). How does a ratio scale differ from an interval scale? Explain using examples (not from the lecture). Please stop here and don’t go beyond this point until we have compared notes on your answers. R &amp; RStudio – Installation Today we start working with R and the first step is to install the program. Please follow these instructions: Go to https://cran.r-project.org/mirrors.html and select a server from which you want to download R. It is convention to do this from the server which is nearest to you. Follow on-screen instructions and install the program. Go to https://rstudio.com/products/rstudio/download/ and download RStudio Desktop which is free. Install the program. Now open RStudio - you do not need to open R itself, as we will be operating it through RStudio. Whilst you need to install both R and RStudio, we will never be working with R directly. Instead, we will be operating it through RStudio. R - Getting Started In this worksheet and also in all other presentations and documents I use on this module, I am using two different fonts: Font for plain text A typewriter font for R functions, values, etc. I am also regularly including “screenshots” of operations in R with their output. Whenever you see these, please replicate them on your own computer. To start, let’s have a look at RStudio itself. When you open the programme, you are presented with the following screen: Figure 1: RStudio It has – for now – three components to it. On the left hand-side you see the so-called Console into which you can enter the commands, and in which also most of the results will be displayed. On the right hands side, you see the Workspace which consists of an upper and a lower window. The upper window has three tabs in it. The tab Environment will provide you with a list of all the data sets you have loaded into R, and also of the objects and values you create (more on that later). Under the History tab, you find a history (I know, who would have thought it) of all the commands you have used. This can be very useful to retrace your steps. In the Connections tab you can connect to online sources. We will not use this tab. In the lower window, you have five tabs. Under Files you find the file structure of your computer. Once you have set a working directory (more on that in a moment), you can also view the files in your working directory here which gives you a good overview of the files you need to refer to for a particular project. The Plots tab will display the graphs we will be producing. Packages form the heart and soul of R and they make the program as powerful as it is (again, more on that later). RStudio also has a Help function, which is rarely very illuminating. I usually search for stuff online on “stackexchange”, as there is a large community of R users out there who share their knowledge and solutions to problems. We won’t use the last tab Viewer. Introduction to R Studio If you can’t get enough of my delightful German accent, then I have some videos for you in which I go through the respective components of the worksheet on screen. Here is the first: RScript If you read the previous section carefully, you will have noticed that I wrote that you can enter the commands” in the Console. You can, but you shouldn’t. What you should be using instead is an RScript. An RScript is a list of commands you use for a project (an essay, your dissertation, an article) to calculate quantities of interest, such as descriptive statistics in the form of mean, median and mode, and produce graphs. One of the foundations of scientific research is “reproducibility”“, or”replicability”. This means that “sufficient information exists with which to understand, evaluate, and build upon a prior work if a third party could replicate the results without any additional information from the author.” King (1995, p. 444, emphasis removed) This principle applies in academia more generally, because only if you understand what a person has done before you, you can pick their work up whether they left it, and push the boundaries of knowledge further. But a bit closer to home, it is also relevant for conducting quantitative research in assessments. We require you to submit an RScript (or a “do file” if you use Stata) now together with your actual essay. This is not only to check what you have done; data preparation is often the most time-consuming part (as you will soon discover), and this is a way to gain recognition for this work. So it is actually to your advantage, and not a mere plagiarism check. The creation of an RScript will allow you to open the raw data, and by running the script, to bring it to exactly where you left off. This saves you saving data sets which can take up a lot of work. If you back the script up properly, you also have an insurance against losing all your work a day before the assessment is due. To create an RScript, click File \\(\\rightarrow\\) New File \\(\\rightarrow\\) RScript. A fourth window opens, and your screen will now look something like this: Figure 2: The RScript Window You can now write your commands in the RScript, where a new line (for now) means a new command. If you want to execute a command, put the cursor on the line the command is on and press “command” / “enter” simultaneoulsy on a Mac and “Ctrl” / “Enter” on Windows. If you precede a line with #, you can write annotations to yourself, for example explaining what you do with a particular command. More on this in the next sub-section. Figure 3 shows the start of the RScript for this worksheet. I prefer a dark background, it’s easier on the eyes, especially when you work with R for long periods. You can change the settings in: Tools \\(\\rightarrow\\) Global Options \\(\\rightarrow\\) Appearance \\(\\rightarrow\\) Twilight. Figure 3: Example of an RScript More Themes If you copy and paste the following code chunks into your “Console” and run one at a time, you will have even more themes4 to choose from: install.packages( &quot;rsthemes&quot;, repos = c(gadenbuie = &#39;https://gadenbuie.r-universe.dev&#39;, getOption(&quot;repos&quot;)) ) rsthemes::install_rsthemes() Appearance RScript Structure Well, I am German, and I like things neat and tidy, so I feel almost compelled to discuss how to properly organise an RScript. But apart from genetical dispositions, a well-organised RScript is also very much in the spirit of reproducibility. It simply makes sense to structure an RScript in such a way that another researcher is able to easily read and understand it. First of all, which commands to include? If you introduce me to your current girlfriend or boyfriend, I have no interest in learning about all your past relationships; they have not worked out. In a similar fashion, nobody wants to read through lines of code that are irrelvant. So you will only include in the RScript those commands which produce the output you actually include in the essay or article. I stated above that if you precede a line with #, you can write annotations to yourself. This is also a useful way to structure an RScript, for example into exercise numbers, sections of an essay /article, or different stages of data preparation (which we will be doing in due course). RScript Structure First Steps in R But enough of the preliminary talk, let’s get started in R. In principle, you can think of R as a massive and powerful calculator. So I will use it as such to start of with. If you want to know what the sum of 5 and 3 is, you type: 5+3 and execute the line as previously explained. In everything that is to follow, commands will be shown in boxes with the output underneath preceded by a number in square brackets. So, including result, the calculation would look like this: 5+3 [1] 8 where the [1] indicates that the 8 is the first component of the result. In this case, we only have one component, so it’s superfluous really, but we will soon encounter situations in which results can have a number of different items. You can copy the code from this page by hovering over the code chunk and clicking the icon in the top-right hand corner. You can then paste it into your RScript. A fundamental component of R is objects. You can define an object by way of a reversed arrow, and you can assign values, characters, or functions to them. If we want to assign the sum of 5 and 3 to an object called result, for example, we call5 result &lt;- 5+3 If we now call the object, R will return its value, 8. result [1] 8 Make a habit of adding a note underneath each code chunk in your RScript (preceded with a #) in which you translate the code into plain English. This is especially useful for the lengthy complex chunks. The Working Directory It is imperative that you create a suitable filing system to organise the materials for all of your modules. At the very least you should have a folder called “University” or similar, in which you have a sub-folder for each module you take. In those modules in which you are working with R, you need to extend this system a little. I have created a schematic of what I have in mind in Figure 4. Figure 4: Folder Structure You see that there is a sub-folder for each week of the module (I have only done three for illustrative purposes), and that each of these folders is divided into lecture and seminar in turn. Into these you can place the lecture and seminar materials, respectively. Create this system now for PO11Q. R works with so-called Working Directories. You can think of these as drawers from which R takes everything it needs to conduct the analysis (such as the data set), and into which it puts everything it produces (such as graph plots). As this will be an R-specific drawer within the seminar, create yet another sub-folder in your seminar folder, and call it something suitable, such as “PO11Q_Seminar_Week 1”. Do NOT call this “Working Directory”, as you will have many of those, rendering this name completely meaningless. Save the file “EU.xlsx” (available on Moodle) into it. Please set up this structure now. If I find you using a random folder on your desktop named “working directory” in the coming weeks, I am going to implode! I mean it. Now we need to tell R to use this folder. If you know the file structure of your computer you can simply use the setwd() command, and enter the path. Here is an example from my computer: setwd(&quot;~/Warwick/Modules/PO11Q/Seminars/Week 5/R Week 5&quot;) If you don’t know the file structure of your computer, then you can click Session \\(\\rightarrow\\) Set Working Directory \\(\\rightarrow\\) Choose Directory. Working Directory R Packages It would be difficult to overstate the importance of packages in R. The program has a number of “base” functions which enable the user to do many different basic things, but packages are extensions that allow you to do pretty much anything and everything with this software - this is one of the reasons why I love it so much. The first package we need to use will enable us to load an Excel sheet into R. It is called readxl. You can install any package with the command install.packages() where the package name goes, wrapped in quotation marks, into the brackets: install.packages(&quot;readxl&quot;) We can then load this package into our library with the library() command. library(readxl) Once you close R at the end of a session, the library will be reset. When you reopen R, you have to load the packages you require again. But you do not have to install them again. Opening your Data Set We are now ready to open a data set in R - where it is called a “data frame”. For this, we create a new object EU, and ask R to read “Sheet 1”” of the Excel file EU.xlsx. Data are taken from European Comission (n.d.). EU &lt;- read_excel(&quot;EU.xlsx&quot;, sheet=&quot;Sheet1&quot;) We can now use our data in R! Loading the Data Set Please do not use the “Import Dataset” button in the Environment, but do this properly, manually. We sometimes need to set options for importing data sets, and the “pointy, clicky” approach won’t be able to offer you what you need. Viewing the Data Unless you have been cheeky and opened the file in Excel to have a look, you have no idea yet, what the data look like. So it’s a good idea to view the data frame before doing anything with it. You can use the View() command to see the data frame: View(EU) If you only want to see the first 6 observations of each variable, use the head() command: head(EU) # A tibble: 6 × 5 country pop18 access area GDP_2015 &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Belgium 11413058 1951 30280 4.66e11 2 Bulgaria 7050034 2007 108560 1.22e11 3 Czechia 10610055 2004 77230 3.19e11 4 Denmark 5781190 1973 42430 2.46e11 5 Germany 82850000 1951 348540 3.60e12 6 Estonia 1319133 2004 42390 3.51e10 If you simply want to know the variable names in the data frame, type: names(EU) [1] &quot;country&quot; &quot;pop18&quot; &quot;access&quot; &quot;area&quot; &quot;GDP_2015&quot; The next one is a very important command, because it reveals not only the variable names and their first few observations, but also the nature of each variable (numerical, character, etc.). It is the str() command, where “str” stands for structure: str(EU) tibble [28 × 5] (S3: tbl_df/tbl/data.frame) $ country : chr [1:28] &quot;Belgium&quot; &quot;Bulgaria&quot; &quot;Czechia&quot; &quot;Denmark&quot; ... $ pop18 : num [1:28] 11413058 7050034 10610055 5781190 82850000 ... $ access : num [1:28] 1951 2007 2004 1973 1951 ... $ area : num [1:28] 30280 108560 77230 42430 348540 ... $ GDP_2015: num [1:28] 4.66e+11 1.22e+11 3.19e+11 2.46e+11 3.60e+12 ... You can see that R has recognised most variables as numerical, one is displayed as a character variable. This is appropriate for some variables, such as pop18, but not for the ordinal variable access which is ordinal. We need to recode it, and all other variables we are unhappy with. Variable Types in R R distinguishes between a number of different variable types and here is a broad overview of them. This will help you in deciding which descriptive statistics to calculate, or into which variable type you need to recode (next step) to achieve what you want. There are two general types: numeric – numbers character (also called string) – letters Within numeric we can distinguish between the following: factor - nominal ordered factor - ordinal integer - numeric, but only “whole” numbers (discrete) numeric - any number (interval or ratio) Numerical variables are already in the data set, we have to attend to nominal and ordinal variables. Nominal Variables In terms of the variable types we encountered in the lecture this week, the country name is a nominal variable. So we need to tell R to turn this into a factor variable. We do this as follows: EU$country = factor(EU$country) Ordinal Variables As mentioned above, the variable access should be ordinal, and therefore has to be turned into an ordered factor. The command which follows is almost identical to producing a factor variable, only that we add the option ordered = TRUE at the end: EU$access_fac = factor(EU$access, ordered = TRUE) If you are familiar with European Studies, you will know that each accession wave has got a particular name. The 1973 enlargement, for example, is called the “First Enlargement”, the 1981 wave the Mediterranean Enlargement, and so forth. Let us create a new variable which uses these names instead of the years. This process is a little more involved, and requires a new package to be installed and loaded: dplyr. This package is part of the so-called tidyverse which is a suite of packages designed to make working with R simpler and commands shorter. You can install all of them by calling install.packages(\"tidyverse\"). We then load the tidyverse with: library(tidyverse) The command which follows takes a little explaining. We start by stating the dataframe we wish to work with, EU. The symbol which follows, \\%&gt;\\%, reads as “and then”, and is called (yes seriously) a pipe. So we take the data frame EU “and then” carry out a function called mutate. This function in turn defines the new variable wave by recoding the variable access_fac. The command then specifies all categories of the “old” variable access_fac and what their respective values in the “new” variable wave are going to be. The categories in each are set in quotation marks, as they are factor / character categories. The last step is then to assign this newly created variable wave to our data frame EU. An alternative procedure, producing exactly the same result is to use the cut() on the access variable which literally cuts up a variable into chunks at the points we specify. This only works on numerical variables which is OK in the present case, as we didn’t change access, and it is still numerical. This also shows you the benefit of always creating a new variable instead of overwriting the original: there is no “back” button in R, if you mess up, you will have the pleasure to start from the beginning. As for the command, we will use a new data frame for this, called EU1 so as not to overwrite the wave variable we have just created (you could also give this variable a new name, but I want to keep the following code analogous to the previous chunk).Again, we use the mutate function, cut the original variable up at the accession years, and specify the levels, this time as labels. Labels denominate the output, whilst level are input. A factor only knows levels which is set by the label function. Here we have already created the levels with the cut() function, and assign labels to these in the second step. EU1 &lt;- EU %&gt;% mutate(wave=cut(access, breaks=c(1950, 1951, 1973, 1986, 1995, 2007, 2013), labels=c(&quot;Founding&quot;,&quot;First&quot;, &quot;Mediterranean&quot;, &quot;Cold War&quot;, &quot;Eastern&quot;, &quot;Balkans&quot;))) levels(EU1$wave) [1] &quot;Founding&quot; &quot;First&quot; &quot;Mediterranean&quot; &quot;Cold War&quot; &quot;Eastern&quot; [6] &quot;Balkans&quot; Recoding a Factor Variable Recoding Ordered Factor Variables Binary Dummy Very often in political science we have yes/no scenarios, such as democracy yes or no, civil war, yes or no, etc. To analyse these scenarios, we can create so-called “dummy variables”. In the present example, let’s specify for each country whether it has been a founding member of the EU. It is a factor variable and so we do this exactly the same way as our initial recoding of the wave variable above: EU2 &lt;- EU %&gt;% mutate(founding = recode(access_fac, &#39;1951&#39;=&quot;Yes&quot;, &#39;1973&#39; = &quot;No&quot;, &#39;1981&#39; = &quot;No&quot;, &#39;1986&#39; = &quot;No&quot;, &#39;1995&#39; = &quot;No&quot;, &#39;2004&#39; = &quot;No&quot;, &#39;2007&#39; = &quot;No&quot;, &#39;2013&#39; = &quot;No&quot;)) str(EU2$founding) Ord.factor w/ 2 levels &quot;Yes&quot;&lt;&quot;No&quot;: 1 2 2 2 1 2 2 2 2 1 ... Sub-Setting Data When we start analysing data, we rarely need all data at the same time. We might not need some variables, at all, for example, or we only want to work with certain observations, such as those countries in the “founding” wave. In these cases, we can subset the data. I will show you some examples of subsetting now, and will be working with the data frame EU2 we created last. By Variable If you are sure you won’t need a variable (remember, there is no back button), you can simply drop (i.e. delete) it. Let’s do this with the area variable: EU2$area &lt;- NULL If we are dropping multiple variables, we can either perform this operation each time, or use another command which allows us to operate with multiple variables at the same time. The select() command comes from the tidyverse package and specifies which variables we wish to keep: EU_pop &lt;- select(EU2, country, pop18, access_fac, founding) We can, however, use the same command and tell R which variables to drop by adding a minus sign in fron of the variables we want to delete. The following command produces exactly the same result as the one before: EU_pop1 &lt;- select(EU2, -access, -GDP_2015) By Observation Instead of dropping and keeping variables, we can do the same thing to individual observations. Here, we use the slice() command (like a cake) and specify which slices we want to drop or keep. For example to drop the Benelux countries we would delete observations 1, 16 and 19: EU_nobenelux &lt;- slice(EU2, -1, -16, -19) Alternatively, if we were only interested in Benelux countries we would subset to only those observations: EU_benelux &lt;- slice(EU2, 1, 16, 19) Keep if a variable has a certain value, e.g. ‘pop18’ larger than 10,000,000 One of the most useful commands is filter(), as it allows us to keep all observations for which the value of a variable is of a particular number. For example if we wanted to conduct an analysis with all countries which have a population in excess of 10 million we could subset by: EU_pop_large &lt;- filter(EU2, pop18 &gt; 10000000) Here is a list of some operators you can use for this purpose: Operator Description &lt; less than &lt;= less than or equal to &gt; greater than &gt;= greater than or equal to == exactly equal to != not equal to !x Not x x | y x OR y x &amp; y x AND y Subsetting Data Ordering Data The data set in its original state is purposely not ordered by any criterion, such as alphabetical order of countries, etc. But we can use R to do exactly that. Let us work with a subset containing only three variables: EU_subset &lt;- select(EU2, country, pop18, access) It would be lovely if the command for ordering data would be called order(), but it is called arrange()6. Let’s order countries by ascending population in a new data frame called eu_order: eu_order &lt;- arrange(EU_subset, pop18) We can now display the first 10 rows with the following command: eu_order[1:10,] # A tibble: 10 × 3 country pop18 access &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Malta 475701 2004 2 Luxembourg 602005 1951 3 Cyprus 864236 2004 4 Estonia 1319133 2004 5 Latvia 1934379 2004 6 Slovenia 2066880 2004 7 Lithuania 2808901 2004 8 Croatia 4105493 2013 9 Ireland 4838259 1973 10 Slovakia 5443120 2004 The content in the brackets refers to the rows (before the comma), and to the columns (after the comma). As we only want certain rows and displaying all variables, I have left the space after the comma blank. We can do the same thing in descending order by calling: eu_order &lt;- arrange(EU_subset, desc(pop18)) eu_order[1:10,] # A tibble: 10 × 3 country pop18 access &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Germany 82850000 1951 2 France 67221943 1951 3 United Kingdom 66238007 1973 4 Italy 60483973 1951 5 Spain 46659302 1986 6 Poland 37976687 2004 7 Romania 19523621 2007 8 Netherlands 17181084 1951 9 Belgium 11413058 1951 10 Greece 10738868 1981 A neat feature of R is that it allows us to order observations by more than one variable. So for example, we could order them by ascending accession wave first, and then by ascending population in 2018 as follows: eu_order &lt;- arrange(EU_subset, access, pop18) eu_order[1:10,] # A tibble: 10 × 3 country pop18 access &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Luxembourg 602005 1951 2 Belgium 11413058 1951 3 Netherlands 17181084 1951 4 Italy 60483973 1951 5 France 67221943 1951 6 Germany 82850000 1951 7 Ireland 4838259 1973 8 Denmark 5781190 1973 9 United Kingdom 66238007 1973 10 Greece 10738868 1981 Grouping Data Looking at the last example, a question that might spring up is in which accession wave the joining countries brought the largest population increase on average to the EU. We can calculate summary statistics for a particular group by, well, grouping them. The first step is to group data into rows with the same value: eu_access &lt;- group_by(EU_subset, access) By the way: whenever you have grouped anything, and finished analysing data in this grouped version it is essential that you ungroup the data afterwards, so that you don’t unintentionally keep using the groups: ungroup(EU_subset) # A tibble: 28 × 3 country pop18 access &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Belgium 11413058 1951 2 Bulgaria 7050034 2007 3 Czechia 10610055 2004 4 Denmark 5781190 1973 5 Germany 82850000 1951 6 Estonia 1319133 2004 7 Ireland 4838259 1973 8 Greece 10738868 1981 9 Spain 46659302 1986 10 France 67221943 1951 # ℹ 18 more rows But let’s calculate the average population size per accession wave in an elegant command which combines multiple steps by using pipes: EU_subset %&gt;% group_by(access) %&gt;% summarise(avg = mean(pop18)) -&gt; eu_popaccess eu_popaccess # A tibble: 8 × 2 access avg &lt;dbl&gt; &lt;dbl&gt; 1 1951 39958677. 2 1973 25619152 3 1981 10738868 4 1986 28475164. 5 1995 8151880. 6 2004 7327746. 7 2007 13286828. 8 2013 4105493 You now see a new variable called avg which contains the average population increase for each wave. In which wave did the joining countries have the largest population on average? Combining Ordering and Grouping Data The question was easy to answer here, as we only have a few accession waves. It starts to get unwieldy though, the more groups we have, but we can let R do the job by combining first grouping, and then ordering. So we take the grouped data frame eu_popaccess and order it by descending avg: eu_popaccess_order &lt;- arrange(eu_popaccess, desc(avg)) eu_popaccess_order # A tibble: 8 × 2 access avg &lt;dbl&gt; &lt;dbl&gt; 1 1951 39958677. 2 1986 28475164. 3 1973 25619152 4 2007 13286828. 5 1981 10738868 6 1995 8151880. 7 2004 7327746. 8 2013 4105493 Saving Please now save this RScript into the same folder (working directory) as the raw data. When R asks before closing, there is no need to save the workspace or the data, as running the RScript on the raw data will bring you precisely to where you left off. Homework For week 7, please finish working through this worksheet. Read the required literature for week 7. Work thoroughly through chapters 7 and 8 of the Fogarty book to make sure you are familiar with all the relevant commands to produce descriptive statistics and graphs with R. Some of the content of this worksheet is taken from Reiche (forthcoming).↩︎ Source: https://www.garrickadenbuie.com/project/rsthemes/↩︎ To “call” means to execute a command.↩︎ There is a command called order(), but it is not part of the tidyverse, and as this package is steadily on the rise in coding, I am only showing you this here.↩︎ "],["reading-week.html", "Reading Week", " Reading Week Reading week is not an institutionalised holiday, but I do expect you to put in about 10 hours of work for this module over the course of this week. Here are a few suggestions for activities to fill these 10 hours: Catch up on the reading Revise the material of Weeks 1-5, as the going will get a little tougher in Week 7. Go through the worksheets of Weeks 5 to make sure you are on top of things with R. Revise all R functions up to this point. You can use the Week 5 Flashcard Section for this. "],["z-scores.html", "z-Scores Self-Assessment Questions7 How to read a z-table Calculations8 Solutions", " z-Scores Self-Assessment Questions7 How do the probability distributions of a discrete and a continuous variable differ? What is a sample distribution? How does the sample distribution differ from the sampling distribution? Why do we need the sampling distribution? What is the difference between the standard deviation and the standard error? Please stop here and don’t go beyond this point until we have compared notes on your answers. How to read a z-table In the lecture you have learned about the Normal Distribution. Under the Normal Distribution, the area under the curve is determined by the number of standard deviations around our mean \\(\\mu\\). This number is expressed in the form of the z-score which is defined as: \\[\\begin{equation} z=\\frac{\\text{Observation} - \\text{Mean}}{\\text{Standard Deviation}}=\\frac{y-\\mu}{\\sigma} \\end{equation}\\] To put this in words, z takes the difference between a particular value we are interested in and the mean. It then divides this distance by the standard deviation, in order to express the distance in units of standard deviations. Why do we do this? We know that under the Normal Distribution the area of the interval mean \\(\\pm\\) one standard deviation is equal to 68%. This is equivalent to the blue area in Figure 5. This also means that the remaining white area is equal to 32%, or the white section on each side 16%. Figure 5: Area under the Normal Distribution Imagine now, we took the point of minus one standard deviation as a starting point, and turn right, as in Figure 6. The white area is still 16%, so that the blue area needs to be 84%. Figure 6: Right-Tail Probability So the probability of finding a value larger than what is equivalent to minus one standard deviation is 84%. We call this a right-tail probability. The beauty is that we can do this for any point on the x-axis. Once we know how many standard deviations a value is removed from the mean, we can use the right-tail probability to assess how likely a value higher (or lower) than this value is to occur. The number of standard deviations is the z-score. Every z-score has a right-tail probability associated with it. These probabilities are listed in the Normal Table (see Moodle). How do we read this Table? Let me take you through the example used in the lecture once more. We assumed that the voter turnout rates of the 2019 European Elections for 28 countries was normally distributed, with \\(\\mu=50.66\\) and \\(\\sigma=16.56\\). You can see this distribution visualised in Figure 7. Figure 7: Voter Turnout in the 2017 European Elections (n=28) The question then was how likely it was for the EU to achieve a voter turnout larger than the voter turnout in the last General Election in the UK (68.8). If we wanted to visualise this, we would need the area to the right of 68.8 on the x-axis. This would look like this: Figure 8: Probability of Voter Turnout &gt; 68.8 In order to calculate the size of this area, we first took the difference between 68.8 and 50.66 which is 18.14. We then divided 18.14 by the standard deviation of 16.56, to express the distance in units of the standard deviation. The result is 1.095411. We know, therefore, that the point of 68.8 percent voter turnout on the x-axis is located 1.095411 standard deviations to the right of the mean. We now need to find the right-tail probability that belongs to this value. In the left-most column of the Normal Table you find the z-values with the first decimal place. Move down to 1.1. From here you turn right, until you hit the second decimal place. As our value is 1.10 you will only have to go one column to the right. If the value was 1.11, you would have to go two columns to the right. For a z-score of 1.1 the area is 0.1357, or 13.57%. We can therefore say that with a voter turnout of 50.66 and a standard deviation of 16.56, the probability of achieving a voter turnout higher than 68.6 is 13.57%. Before moving on, I need to note that z can be negative. If we were assessing the probability of achieving a voter turnout of less than 32.52% (50.66-18.14), we would get a z-score of -1.1. Because the Normal Distribution is symmetrical, we can use the same process, but need to reverse the logic. Because the right tail probability gives us the area to the right of the z-score, a negative z-score would give us the area to the left of the z-score. So the probability of a voter turnout smaller than 32.52 is also 13.57%. With this knowledge at hand, let’s do some calculations. Calculations8 The mean weight of a bag of apples is 1 kg. The weight of bags is normally distributed around this mean with a standard deviation of 50g. Billy is looking for the heaviest bag possible and finds one that is 1082 g. What is the probability of finding a heavier bag? What is the probability that Billy will find a bag lighter than 870g? How would the results of a. and b. change if the standard deviation was only 40g? Why? Solutions You can find the Solutions in the Downloads Section. Some of the content of this worksheet is taken from Reiche (forthcoming).↩︎ These are taken from Reiche (forthcoming).↩︎ "],["working-with-r.html", "Working with R Preliminary Stuff Descriptive Statistics Graphs Basic Graphs Advanced Graphs Even More Advanced Graphs Organising Code in the RScript Exercises Captions for Tables and Figures in Word Homework", " Working with R Preliminary Stuff Create a working directory (folder) for Week 7, create an RScript for Week 7, and set the working directory. In my case: setwd(&quot;~/Warwick/Modules/PO11Q/Seminars/Week 7/Worksheet&quot;) Load the data set Example.xlsx (taken from European Comission (n.d.)) into R: library(readxl) EU &lt;- read_excel(&quot;EU.xlsx&quot;, sheet=&quot;Sheet1&quot;) Now copy/paste the entire RScript from Week 5 into the Week 7 RScript. Run the entire script by highlighting everything (“command”/“a” on a Mac and “ctrl”/“a” on Windows) and executing. You should now have a completely and correctly coded data frame that is ready to work with in your R Workspace. If not, give me a shout to sort out problems. Descriptive Statistics We have covered quite a large number of descriptive statistics, so far. These are: Mean Median Mode Standard Deviation Variance Quartiles and Percentiles Range Interquartile Range They are a lot of effort to calculate by hand, especially for larger data sets, but R can do these with a few intuitive commands. First up is the mean. mean(EU$pop18) [1] 18311106 Then the median: median(EU$pop18) [1] 9300319 You can get information on the quartiles (remember that the median is the second quartile), the mean, as well as the minimum and maximum through one, simple command: summary(EU$pop18) Min. 1st Qu. Median Mean 3rd Qu. Max. 475701 3781345 9300319 18311106 17766718 82850000 Let’s now move to measures of variability. First up is the range; you can either calculate this with two commands by finding out the minimum and maximum separately, or just ask R to give you both values straight away: min(EU$pop18) [1] 475701 max(EU$pop18) [1] 82850000 range(EU$pop18) [1] 475701 82850000 The stadard deviation is rather long-winded to calculate by hand, but the R command is short and sweet: sd(EU$pop18) [1] 23787945 As you know, the variance is the squared standard deviation, but you can calculate it with its own command in R, too: var(EU$pop18) [1] 5.658663e+14 Descriptive Statistics Now that you have all the relevant tools at hand, complete the following tasks: Generate descriptive statistics for 3 of our variables. Recode the variable ‘GDP_2015’ into a ordered factor called ‘gdp_level’ with three levels called “low”, “medium”, and “high” with cut-off points of your own choosing. Produce a tabulation for ‘gdp_level’. Graphs R is probably the most powerful statistics programme for creating graphs. As this is an introductory level module, and we only have so much time available in the seminars, I will only be able to introduce you to the most commonly used ones; in the first instance histograms and boxplots. I will then introduce you to the package ggplot2 which is simply the best invention since sliced bread, as it gives you pretty much endless optionality in customising graphs to show exactly what you want. Whenever you produce a graph and you use it in an essay, your dissertation, or article, it is crucial that the graph is able to communicate its message independently from the text. So, a reader should be able to understand the graph and be able to appreciate fully its message without having to read the text. In a similar fashion, the text should always be written in such a way that a reader is able to understand it without having to look at the graph. This is a principle which equally applies to tables (more on this on PO12Q). If you do not follow this principle in the assessments on my modules, you will be marked down. Chapter 4 in “The Visual Display of Quantitative Information” by Tufte (2001) is on the reading list as an essential item, but there are some more principles he sets out at the start of the book (p. 13) which are worthwhile repeating here: Excellence in statistical graphics consists of complex ideas communicated with clarity, precision, and efficiency. Graphical displays should show the data induce the viewer to think about the substance rather than about methodology, graphic design, the technology of graphic production, or something else avoid distorting what the data have to say present many numbers in a small space make large data sets coherent encourage the eye to compare different pieces of data reveal the data at several levels of detail, from a broad overview to the fine structure serve a reasonably clear purpose: description, exploration, tabulation, or decoration be closely integrated with the statistical and verbal descriptions of a data set. Graphics reveal data. Indeed graphics can be more precise and revealing than conventional statistical computations. Basic Graphs Let’s start with a histogram of the variable pop18. The range of the pop18 variable is about 82 million - this is rather unwieldy to imagine and also to put onto axes of graphs, as they would mostly consist of zeros. So let’s express the population of each countries in million instead: EU$popmio &lt;- EU$pop18/1000000 We can now produce a histogram. Before doing this, it is sensible to think about the number of bars we want in the histogram. The smallest country has just shy of 500,000 inhabitants, whereas the largest has over 82 million. So, I would like the x-axis to run from zero to 100 (million) and divide this into 5 bars. Accordingly, we are introducing 4 breaks on the x-axis with the following command: hist(EU$popmio, breaks=4) This is certainly a histogram, but it does not conform to the principle of graphs that they should be able to communicate their message independently, yet. Take the label of the x-axis, for example, what does EU$popmio mean? You and I know, but somebody who doesn’t know R language wouldn’t. We can tell R to adjust the axis label, as well as the main title of the histogram as follows: hist(EU$popmio, breaks=4, xlab=&quot;Population in million&quot;, main=&quot;Histogram of EU Population (2018)&quot;) Histograms This is fine now. Ugly, but fine. I will show you how to do a boxplot next, and then I will take you through the process of making all this look a bit more jazzy. The command for the boxplot is very intuitive. The default in R is to arrange the boxplot vertically. I prefer them horizontally, and you can set this in an equally intuitive option. boxplot(EU$popmio, horizontal = TRUE) You will recognise the descriptives we calculated earlier with the summary() function: summary(EU$popmio) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.4757 3.7814 9.3003 18.3111 17.7667 82.8500 Explain the outliers on the right mathematically. Advanced Graphs The graphs we have produced so far are functional, but let’s be honest, they wouldn’t win any beauty contests. There is, as mentioned earlier, an amazing package called ggplot2 which changes this dramatically. You have already installed it as a part of the tidyverse. Otherwise, the function would be: install.packages(&quot;ggplot2&quot;) We can just load it: library(ggplot2) The “gg” in ggplot2 stands for “grammar of graphs”. You will be familiar with the term “grammar”” from learning a language already. In this context, we use grammar to build sentences by choosing and arranging a variety different components, such as subjects, verbs and objects. If you know how to do this properly, you can express exactly what you want to say. The grammar of graphs adopts this logic and specifies a number of different components which allow you to create a graph which is able to communicate exactly what you wish to show. ggplot2 has eight basic grammatical arguments: Table 2: GGPLOT Components Data Frame The data you wish to visualize. Aesthetic Mappings Here you specify how the data are assigned to colour, size, etc. For now, this is the variable for which we want to create a graphical distribution. Geom Short for “geometry”. Use a geom function to represent data points through geometric objects, such as points, lines, etc. Each function returns a layer. Stat You can include statistical summaries through this, such as smoothing, or regression lines. Position Position adjustments determine how to arrange geoms that would otherwise occupy the same space. Facets Facets divide a plot into sub-plots based on the values of one or more discrete variables. Scale Maps data values to the visual values of an aesthetic. For example female=pink, male=blue. Coordinates How do the numbers get translated onto the plot? We are not going to look at this on this module. I like to think of using these arguments like dressing myself in the morning. The minimum that common decency requires me to wear if I wish to leave the house is some underwear, some trousers, and a top. Depending on how I feel and what the weather is like, I can add more layers, like socks, a jumper, or a scarf. It is exactly the same with ggplot2. As a minimum to produce a plot you need a data frame, the aesthetic mapping and a geom. Once you have produced this minimalistic graph, you can modify it, by adding more components / arguments. As you can imagine the possibilities are almost endless, and we only have time to deal with the minimum here. This is not a problem, however, as most of the other grammatical arguments (Stats, Position, Facets and Scales) generally have sensible defaults. So how does this work in practice? Let us reproduce the histogram of the age variable. We start by calling ggplot2 and advise the function which data frame we wish to use (EU). In a second step, we add a geometry – in our case geom_histogram. Within the geometry, we need to specify for which variable we wish to create a distribution, or in the language of ggplot2 which variable we wish to map to the geom as our Aesthetic. To produce 5 bars again, we specify a bandwith of 20 million (this refers to popmio). ggplot(data=EU) + geom_histogram(mapping=aes(popmio), binwidth = 20) Annoyingly, ggplot places the axis ticks in the middle of each bar which is WRONG for histograms. They need to align with the boundaries of the bars. We do this by telling R the boundary of the plot: ggplot(data=EU) + geom_histogram(mapping=aes(popmio), binwidth = 20, boundary = 0) This has shifted the ticks to the left, but now R has decided to label the x-axis in steps of 25, whereas our bars have a bandwidth of 20. Once again, we have the variable name on the x-axis, instead of a label which anybody could understand. I also prefer “Frequency” on the y-axis, instead of “Count”. To address both of these concerns, we simply add a layer for each. First up are the axis ticks. Our variable is continuous, so we choose the scale_x_continuous option, and tell R to break the axis up into a sequence which starts with zero, ends at 100 and has steps of 20 in between. In the labs argument we adjust the labelling as intended: ggplot(data=EU) + geom_histogram(mapping=aes(popmio), binwidth = 20, boundary = 0) + scale_x_continuous(breaks = seq(0, 100, 20)) + labs(x=&quot;Population (in million)&quot;, y=&quot;Frequency&quot;) + theme_classic() I have also removed the background in line with the principles set out by Tufte (2001, p. 96) by adding theme_classic(). This is it. A graph which can communicate its message independently, and which looks aesthetically pleasing. In the present case we have the population, so displaying the frequency on the y-axis is sort of sensible, but usually we would be dealing with a sample. Here the count is not very telling and we would be using percentages, instead. Let’s do it! Even More Advanced Graphs Unfortunately, there is no easy, default way to do this in R, but necessitates a calculation within the ggplot command. Once more we call ggplot and use the EU data set, and select the geom geom_histogram. Again we specify the binwidth as 20 with a boundary of zero, and put popmio on the x-axis. Now comes the point where we need to do something new, because y is not equivalent to the frequency any more, but should be percentage. To achieve this we advise R to put the density there (which is the relative frequency from the the lecture in week 5), and multiply this density by 100 to get percentage. Nothing has changed on the scaling of the x-axis from the previous plot, so we can copy and paste the scale_x_continuous section, as well as the labelling of the x-axis. In this last step, we now also need to adjust the label of the y-axis, because this has now percentage on it, and not frequency. The result is this: ggplot(data = EU) + geom_histogram(binwidth = 20, boundary = 0, aes(x= popmio, y = (..count..)/sum(..count..)*100)) + scale_x_continuous(breaks = seq(0, 100, 20)) + labs(x=&quot;Population (in million)&quot;, y=&quot;percent&quot;) + theme_classic() Jazzy Graphs with GGPLOT Organising Code in the RScript Now is probably a good time to make you aware of how I have been organising code which runs over several lines. I could also have written the code of the last graph as ggplot(data = EU) + geom_histogram(binwidth = 20, boundary = 0, aes(x= popmio, y = (..count..)/sum(..count..)*100)) + scale_x_continuous(breaks = seq(0, 100, 20)) + labs(x=&quot;Population&quot;, y=&quot;percent&quot;) + theme_classic() but this would have made it rather difficult to disentangle and to spot the structure of the graph straight away. So it is also a good idea to structure the code in a logical way which allows a reader to understand it as easily as possible. R is very smart in the way it indents the next line after pressing “enter” in an RScript automatically to the appropriate position. You see for example that in ggplot(data = EU) + geom_histogram(binwidth = 20, boundary = 0, aes(x= popmio, y = (..count..)/sum(..count..)*100)) + scale_x_continuous(breaks = seq(0, 100, 20)) + labs(x=&quot;Population (in million)&quot;, y=&quot;percent&quot;) + theme_classic() the aes which belongs to the geom_histogram layer is indented just so it starts flush with the first argument (binwidth) within this layer. Exercises Using these commands, and moving beyond with the help of today’s reading, complete the following tasks: Produce two base-R graphs of different types (e.g. histogram, bar chart, box-and-whisker plot) for separate variables in the EU data set. Produce two ggplot graphs of different types (e.g. histogram, bar chart, box-and-whisker plot) for separate variables in the EU data set. Google to find more geoms. Captions for Tables and Figures in Word In essays, your dissertation and in articles, you will have to refer to tables and figures in the text. Now, you can do this by writing “the figure below”. But this is not very elegant. Also, what happens if you change the layout and all of a sudden “the figure below” becomes “the figure above”. This not only causes additional work because you have to edit the text and check all references to tables and figures once you are done (which is tedious beyond description), but there is also the risk that you miss one or a few in the process. MS Word has a nifty function that allows you to insert captions for figures and tables, and then to insert cross-references into the text which get updated automatically before you send the document to the printer. Here is how to do it: Say, you have a figure inserted into Word. You now click on it, then hover over the bottom right-hand square, and right-click with your mouse. From the resulting context menu you select “Insert Caption”. This results in the following window: Select whether the item you want to describe is a figure, or a table. Then make sure you place the caption “below” the item (this is default). Then type your caption into the box at the top, such as “Figure 1: Skewness of Distributions”. Make sure the caption is telling. The reader needs to know from the caption what the figure or table is about. When you click OK, the document looks like this: Now you start writing the text and come to the point where you refer to the figure in question. Here, all you have to do is to select “Insert” and “Cross-Reference”” and select the following options in the pop-up window: Your text will then look like this: You don’t have to worry now about the sequence of numbering any more. If you insert another figure above this one, and insert a cross-reference in the text again, the sequence is automatically updated and our former “Figure 1” becomes “Figure 2”. Tables and figures have separate sequences of numbering. One last word on the display of data in tables: DO NOT screenshot tables from R and insert them into your presentations. They look ugly and unprofessional. Make the effort and create a proper table, either in Word or Excel and populate it manually with the data from R. The insertion of captions and cross-references is the same as described above. Homework Answer the following questions, using the data set from week 5 and submit the RScript on Moodle (find the link under Seminar for Week 8) by 5PM on Friday, 17 November: Complete the base-R and ‘ggplot’ graphs from the Exercises in Section 7. Calculate GDP per capita in a new variable, and sort countries in descending order (for this it is ok to mix 2015 and 2018 data). Which country is the least and which is the most densely populated? Recode the ‘pop18’ variable into a categorical variable with low, medium, and high population (cut-off points are your choice). "],["confidence-intervals.html", "Confidence Intervals Self-Assessment Questions9 Calculating Confidence Intervals Exercises Solutions", " Confidence Intervals Self-Assessment Questions9 How do we interpret a confidence interval? What does the central limit theorem postulate? Why do we need it? Why do we need a t-distribution? What is the difference between the normal distribution and the t-distribution? Please stop here and don’t go beyond this point until we have compared notes on your answers. Calculating Confidence Intervals Just as a reminder, what is a confidence interval? Confidence Interval A confidence Interval for a parameter is an interval of numbers within which the parameter is believed to fall. The probability that this method produces an interval that contains the parameter is called the confidence level. This is a number chosen to be close to 1, such as 0.95 or 0.99. (Agresti, 2018, p. 110) There are two scenarios under which we are calculating confidence intervals: (1) we know \\(\\sigma\\) (the standard deviation of the population distribution), and (2) we don’t know \\(\\sigma\\). Let me take you through these two scenarios in turn: 1. We know \\(\\sigma\\) If we know the population distribution, then we can use z. Assume we have sample data on the age of students in the university with n=81, and \\(\\bar{y}=26\\). The standard deviation of the population (\\(\\sigma\\)) is 9. We now want an interval within which we find with 99% certainty the true average age of students (we did this example with 95% in the lecture). We start with calculating the standard error: \\[\\begin{equation*} \\sigma_{\\bar{y}} = \\frac{\\sigma}{\\sqrt{n}} \\end{equation*}\\] \\[\\begin{equation*} \\sigma_{\\bar{y}} = \\frac{9}{\\sqrt{81}} = 1 \\end{equation*}\\] What is important to bear in mind for the construction of confidence intervals, is that we do not just need the right-tail probability, because the area we are trying to cover under the distribution is symmetrical around the mean. This is visualised in Figure 9 where the area is defined between \\(\\pm\\) 1.96 standard deviations around the mean. Figure 9: 95 Percent Confidence Interval around the Mean As the grey area needs to be 95%, the white areas to the left and right need to be equal to 5% jointly. So each of them is 2.5%. When we look into our Table with right-tail probabilities, we therefore need to look for the z-score that corresponds to 0.025 (or 2.5%). When you look in the Normal Table (see Statistical Tables), then you will find this at z=1.96. Now, the question arises, how many standard deviations we need for a 99% confidence interval. The area to the left and right needs to be jointly 1%, or 0.05% each side. We consult the Normal Table again, and try to find the z-score for 0.005. The exact value is not available, only either 0.0051, or 0.0049. 0.0051 would lead to a confidence interval of 98.98%, so not quite large enough. We therefore need to go for 0.0049, and the corresponding z-score of z=2.58. \\[\\begin{equation*} \\bar{y} \\pm 2.58 \\times \\sigma_{\\bar{Y}} \\end{equation*}\\] Popping the values in we receive \\[\\begin{equation*} 26 \\pm 2.58 \\times 1 \\end{equation*}\\] As a result, we can say that with 99% certainty the true average age of students at Warwick lies between 23.42 and 28.58. Note that this confidence interval is wider than the 95% one where the boundaries were defined as 24.04 and 27.96. As we went for higher certainty here, the confidence interval became wider. If we want our interval to contain the true average in 99 out of 100 samples, we need to cast our net wider than if we were content with 95. 2. We don’t know \\(\\sigma\\) If we don’t know the population distribution and its standard deviation, we need to use the t-distribution. As you know from the lecture, the t-distribution is a shape shifter. Its width depends on the degrees of freedom: the more degrees of freedom we have, the more narrow, or the closer to the normal distribution it comes. With df=30 the shapes of the t-distribution and the normal distribution are almost identical. You can see this in the following Figure, comparing the yellow t-distribution for \\(df=30\\) and the black (normal) distribution. Figure 10: Comparison of t-Distributions In reversed logic, this also means that the t-distribution is rather wide for small sample sizes (and small df). Consequently, a confidence interval of a given level (e.g. 99%) would be much wider for a small sample size than under the normal distribution (or t-distributions with higher sample sizes). Let me illustrate this using the same sample average as for the normal distribution example above, \\(\\bar{y}=26\\). We have a sample standard deviation (\\(s\\)) of 2. Our sample size is very small, we only have \\(n=4\\). Again, we want an interval within which we find with 99% certainty the true average age of students. We start by estimating the standard error: \\[\\begin{equation*} se=\\frac{s}{\\sqrt{n}} \\end{equation*}\\] \\[\\begin{equation*} se=\\frac{2}{\\sqrt{4}}=1 \\end{equation*}\\] This time we have to search in the t-Table, taking into account the degrees of freedom. As \\(n=4\\), this means that \\(df=3\\). Conveniently, the Table lists at the top the desired confidence interval. For \\(df=3\\) and a 99% confidence interval, the corresponding t-value is 5.841. Once again, we calculate: \\[\\begin{equation*} \\bar{y} \\pm 5.841 \\times se \\end{equation*}\\] and \\[\\begin{equation*} 26 \\pm 5.841 \\times 1 \\end{equation*}\\] The resulting boundaries are 20.159 as the lower boundary, and 31.841 as the upper boundary. This is far wider than the 99% confidence interval we had for the normal distribution, where the lower and upper boundaries were 23.42 and 28.58, respectively. This is a reflection of the fact that we only had a very small sample (\\(n=4\\)) to base our inference on and therefore have a lot of uncertainty in our inference. To conclude, it is fair to say that the procedure is essentially the same as with the normal procedure, only that have to go the extra step of taking into account the degrees of freedom. You are now ready to do some exercises. Exercises Conceptual Exercises10 The number of diners visiting a restaurant on a Thursday is normally distributed with a mean of 150 and standard deviation of 30. One Thursday only 100 people eat in the restaurant, and the manager says, “next week will be better”. What is the probability she is right? The number of diners on a Friday is also normally distributed with a mean of 200 and a standard deviation of 50. Which two values, symmetrical around the mean contain the number of Friday diners 80% of the time? A researcher is analysing individuals’ relative fear of being a victim of burglary on a 1-100 scale. This variable is normally distributed. A random sample of 9 individuals found a mean score of 47 on the scale with a sample variance of 158.76 for fear of being burgled. What distribution would be used to calculate an 80% confidence interval around this mean? Construct that interval. Is this a suitable sample size for seeing whether individuals are more nervous around burglary or murder, which is found to have an 80% confidence interval between 2.76 and 14.65? We are investigating the height of men in the UK. For this we have obtained a random sample of 100 UK men and found they had a mean height of 180cm with a standard deviation of 10cm. Construct a 95% confidence interval for the mean height of UK males. Select all true statements concerning the constructed confidence interval and justify your choice for each statement. The probability of the population mean being within the upper and lower bounds is 95%. 95% of men’s heights fall between the upper and lower bound. 95% of the cases in the sample fall between the upper and lower bound. On average 95% of confidence intervals constructed would contain the population mean. On average 95% of the means of samples with 100 respondents will fall within the upper and lower bands. On average 95% of the sample means equal the population mean. R Exercises Introduction For the purpose of this worksheet you will be using a replication data set from Dorff (2011) which provides replication data for the famous study of the determinants of civil war by Fearon &amp; Laitin (2003). This is the abstract, taken from the article: An influential conventional wisdom holds that civil wars proliferated rapidly with the end of the Cold War and that the root cause of many or most of these has been ethnic and religious antagonisms. We show that the current prevalence of internal war is mainly the result of a steady accumulation of protracted conflicts since the 1950s and 1960s rather than a sudden change associated with a new, post-Cold War international system. We also find that after controlling for per capita income, more ethnically or religiously diverse countries have been no more likely to experience significant civil violence in this period. We argue for understanding civil war in this period in terms of insurgency or rural guerrilla warfare, a particular form of military practice that can be harnessed to diverse political agendas. The factors that explain which countries have been at risk for civil war are not their ethnic or religious characteristics but rather the conditions that favor insurgency. These include poverty – which marks financially and bureaucratically weak states and also favors rebel recruitment – political instability, rough terrain, and large populations. The fearon dataset contains several variables for each country in 1994, whilst the fearonfull data set contains data for all years from 1945 to 1999. Not all of the operations in the exercises were covered by the material in the lectures / seminars on PO11Q. Some of these rely on the reading, and for some you need to google for help on stackoverflow. You can find a full codebook in Table 3. Table 3: Codebook to Fearon Data Sets Variable Label year 1945-1999 war 0 (no); 1 (yes) ef ethnic fractionalisation (%) relfrac religious fractionalisation (%) pop population (in 1000s) mtnest mountainous terrain (%) elevdiff elevation difference of the highest and lowest points (metres) polity2 regime score: -10 (autocracy) to 10 (democracy) gdpen GDP per capita (in 1000s, 1985 US Dollar) Oil less than 1/3 export revenue from oil (0); 1/3 or more export revenue from oil (1) plural share of largest ethnic group (%) Exercises These exercises are deliberately slightly more advanced than those of previous weeks to push you a little. For some of these you will have to use google, as we have not covered all the necessary functions on the module, yet. Produce a new data frame of df1, df2, df3, df4. For each dataset, arrange them according to countries with: Lowest Ethnic Fractionalisation Highest Population Highest mountainous terrain Lowest elevation Create a data frame which looks upon countries that used to be British colonies. Transform the British colony variable into a binary factor of “No” and “Yes”. Make a dummy variable which shows whether or not a country is majority Muslim, given that the requirement of a majority Muslim country is more than 50% of its population. Do not forget to order the factors. Use cut() Use ifelse() The Polity IV score measures democracy on a scale from -10 to +10 where -10 is equal to perfect autocracy and +10 equal to perfect democracy. Often, the democratisation literature distinguishes between autocracies, anocracies and democracies. We can achieve this differentiation in the Polity IV score as follows: Autocracies: -10 to -6 Anocracies: -5 to +5 Democracies: +6 to +10 Using this categorization of polity score; transform the numerical values of polity scores as follows: Into an ordered factor Into a binary dummy variable of Democracy/Non-Democracy where only the level “Democracy” of the previous step remains as “Democracy”. Find the mean difference between: the share of largest ethnic group and second largest ethnic group. the share of largest ethnic group and second largest ethnic group within former British colonies (using previous exercise no. 4). Let’s do some more recoding: Create an age-group dataframe with the following categories: NA 0-18 18-35 35-50 50-70 Transform the values into numeric values, and separate into a lower and upper category Create a column - Find the mid of each levels Create a column - Find the interval of each levels For this exercise use the data frame fearonfull which contains data for all years between 1945 and 1999. Population is defined in terms of 1,000s. Create a new data which consists of countries with more than 10000000 people. (large population) Find the average of population of each large population country from 1945-1999. Hint: group according to country, then find the mean. Sub-setting data, using the fearonfull data frame: Extract the necessary variables to compare the social fractionalization between countries. Retain the last row of the dataset. Filter the dataset with only countries in an ongoing war (variable ended). Find the country with an ongoing war in 1999. Generate a dataframe and find all countries with GDP per capita greater than the world’s mean in 1999 (Function hint: filter) Generate a dataframe that only consist of data from 1998 and 1999. Change the data format into a wide data using spread for the observations of GDP per capita (you need to use fearonfull again). Generate a data frame consisting of list of countries and their populations in 1945 and 1995. Find the mean of population differences between 1945 and 1995. Find the mean of population differences between 1945 and 1995 (you need to use fearonfull again). Calculate the average GDP per capita each oil-producing country has had between 1945 and 1999. Which has the highest mean of GDP per capita (you need to use fearonfull again)? For this exercise use the fearonfull data set. Check how many missing values are in the dataset as a whole and in the polity2 variable. Omit rows with missing values in GDP per capita and population using the filter function. Use the data in exercise number 13. Calculate the averages of GDP per capita and population per country between 1945 and 1999. Find the mean GDP per country. Filter the top 10 highest GDP countries Find the difference between each country’s largest ethnic group and second largest ethnic group. Arrange the countries in ascending order based on the difference Solutions You can find the Solutions in the Downloads Section. Please note that the conceptual exercises have a document in the “Documents” Section, whereas the solutions to the R Exercises are in an RScript in the section of the same name. Some of the content of this worksheet is taken from Reiche (forthcoming).↩︎ These are taken from Reiche (forthcoming).↩︎ "],["significance-testing.html", "Significance Testing Group Work – Self-Reflection11 R Exercises Solutions", " Significance Testing Group Work – Self-Reflection11 What does a significance test do? Why can we not sacrifice the randomisation assumption in significance testing? Explain the difference between a significance test and a confidence interval. Explain the relationship between the desired \\(\\alpha\\) level and the Type I and II Errors. What are the differences between a one- and a two-sided significance test? Give an example for each. Please stop here and don’t go beyond this point until we have compared notes on your answers. R Exercises These exercises will use the ks2.csv dataset. This data comprises fictitious12 average grades of Key Stage 2 (KS2) students in the UK, with 1,980 KS2 students’ test scores being included for reading (reading), mathematics (maths), and grammar, punctuation, and spelling (gps), as well as the mean of these three test scores (avg\\_all). The test scores have been standardised, with 80 representing the lowest possible mark, 120 representing the highest possible mark, 100 representing the minimum passing mark, and -1 representing an ungraded test. Load the ks2.csv dataset into R. Remove any observations that have any ungraded test scores. Calculate the means and standard deviations of each of the three subjects. Write a brief description of the insights that can be drawn from these values. Conduct a t-test to see if the means of each of the three subjects’ marks are statistically different from 100 at the 95% confidence level. Identify three ways that suggest statistically significant or insignificant differences. Conduct a t-test to see if the mean of the average score of the three tests is statistically less than 105 at the 99% confidence level. Interpret the results. The following questions will look at students’ English abilities generally. Create a new variable called english, which consists of the average of the reading and grammar, punctuation, and spelling variables. Conduct a t-test to see if the mean of the average score of the new English variable is statistically less than 105 at the 99.9% confidence level. Interpret the results. Conduct a t-test to see if the mean of the average score of the new English variable is statistically different from 105 at the 99.9% confidence level. Interpret the results. Is the any difference in the interpretation between the two above tests? Are there any differences in the results? Why? You are tasked with investing the performance of students who passed in mathematics those who did not. For the following tests, use a 95% confidence level. Create a binary variable that has two categories: those who passed mathematics (100 \\(\\leq\\) mark) and those who failed mathematics (mark \\(&lt;\\) 100). Conduct a proportion test to see if the proportion of students that fail mathematics is 10% or greater. Interpret the results. Conduct a t-test on the group who fail mathematics to see if they, on average, have marks for English statistically less than 100. Interpret the results. Conduct a t-test on the group who pass mathematics to see if they, on average, have marks for grammar, spelling, and punctuation statistically greater than 105. Interpret the results. Now it is worth investigating how students who fail at least one subject perform. Create a binary variable that has two categories: those who passed all three subjects (all marks greater than or equal to 100) and those who failed at least one subject (one or more marks less than 100). It can be hypothesised that the group of students who failed will have a mean of all of the test marks significantly below the pass mark of 100. Test this and interpret the findings with respect to the statistical and practical significance of the test. Imagine you are part of a team work working within the Department for Education, tasked with investigating this sample to produce recommendations for policymakers. Normalise the variable that contains the average of all three marks by setting the lowest mark (80) to 0, the highest mark (120) to 100, and the minimum pass mark (100) to 50. Justify why it might be useful to normalise these marks to this scale for non-specialist policymakers. Construct a categorical variable that consists of five categories: 0-49.99 (Fail), 50-59.99 (Pass), 60-60.99 (Merit), 70-79.99 (Distinction), and 80+ (Distinction+). Answer the following questions but write your answers as if intended for a non-specialist policy making with no knowledge of statistics: Are the averages of the Pass, Merit, and Distinction groups different from their middle marks (55, 65, and 75, respectively)? If so, which direction? Is the average mark of the Distinction+ group lower than the maximum mark of the group? Is the mean mark of the Fail group higher than the median mark of the group? Which skew does this indicate in the distribution? Solutions You can find the Solutions in the Downloads Section. All exercises are a reproduction from Reiche (forthcoming).↩︎ The means are based on KS2 scaled score averages which have been averaged over 2016-2019.↩︎ "],["introduction-to-causality.html", "Introduction to Causality Self-Assessment Questions Exam Preparation", " Introduction to Causality Self-Assessment Questions How do causality in every day life and causality in statistics differ? Explain the role of symmetry and asymmetry in causality. Give an example for a counterfactual. Why is the historical context of the analysis important for establishing causality? How do the attributes of causality relate to the topics on PO11Q? Please stop here and don’t go beyond this point until we have compared notes on your answers. Exam Preparation We will go through the solutions to the mock exam today. Please prepare this exam and note questions about the module content which we will discuss. "],["downloads.html", "Downloads Documents Data Sets R Scripts", " Downloads Yes, all solutions and all RScripts are available to you without silly time restrictions. The reason is that I have come to realise that I am operating a module in a university, and not in a kindergarten. Of course, you can download and look at the solutions before you have given the exercises a go yourself first. By all means, cheat. But I can’t promise you that you will learn very much. It’s your choice. Documents PO11Q Bibliography Statistical Tables QS_POQ_Essay and Dissertation Guidelines Week 4 Worksheet Solutions Week 7 Worksheet Solutions Week 8 Worksheet Solutions Data Sets EU.xlsx fearon.dta fearonfull.dta ks2.csv R Scripts Week 8 Solutions Week 9 Solutions "],["list-of-references-1.html", "List of References", " List of References Agresti, A. (2018). Statistical Methods for the Social Sciences (Fifth). Harlow: Pearson. Dorff, C. (2011). Replication data for: Ethnicity, Insurgency, and Civil War by Fearon and Laitin. V2. Harvard Dataverse. European Comission. (n.d.). Eurostat – Your Key to European Statistics. available online at https://ec.europa.eu/eurostat/data/database. Fearon, J. D., &amp; Laitin, D. D. (2003). Ethnicity, Insurgency, and Civil War. The American Political Science Review, 97(1), 75–90. Fogarty, B. J. (2023). Quantitative Social Science Data With R (Second). Thousand Oaks, CA: Sage. King, G. (1995). Replication, replication. PS: Political Science and Politics, 28(3), 541–559. Reiche, F. (forthcoming). Introduction to Quantitative Methods in the Social Sciences. Oxford: Oxford University Press. Stimson, J. A. (n.d.). Professional Writing in Political Science: A Highly opinionated Essay. available online at http://stimson.web.unc.edu/files/2018/02/Writing.pdf. Tufte, E. R. (2001). The Visual Display of Quantitative Information (Second). Cheshire, Conn: Graphics Press. "]]
